# Methods

We built various linear models from the **Credit.csv** data set. We predicted the variable 'Balance' in terms of ten predictors: 'gender', 'student', 'status', and 'ethnicity', 'age', 'cards', 'education', 'income', 'limit', and 'rating'. 

We created 5 different scripts for running various regression methods: Ordinary Least Squares, Ridge regression, Lasso Regression, Principal Components Regression, and Partial Least Squares Regression.

Our first method, was a simple linear model, Oridnary Least Squares. In **ols_regression.R**, using the lm() where the y variable is Balance and the x variable is the combination of the the rest of the variables, the results from the OLS regression methods are output. This script's outputs will be used as a basis for all other regression models to compare to. We read in the scaled data set, which in turn is used for the regression model.

Our next two methods, Ridge and Lasso regression, are shrinkage methods. The script, **ridge-script.R**, performs a ridge regression and **lasso_regression.R** performs a lasso regression. The steps for these two methods are nearly identical:

1. Load **scaled_credit.csv**, and 'library(glmnet)' and set the seed, for reproducibility sake.

2. Create a 'x' and 'y' varible from the training set, read in from my Rdata file containing training and testing set indicies. 

3. Run 'cv.glmnet()' which performs 10-fold cross-validation and outputs an intercept term and standardizes the variables by default. For the function arguements use the 'x' and 'y' from above, 'lambda = 10^seq(10, -2, length = 100)', 'intercept = FALSE', and 'standardize = FALSE' because our **scaled_credit.csv** is already standardized. For ridge regression we use 'alpha =0' and for lasso we use 'alpha =1'.

4. 'cv.glmnet()' will output a list of models. We decide the best one based of the minimum lambda and then save this lambda value as well as the coefficients associated with it.

5. Next we plot the model and save it to a png.

6. Once we identified the best model we use the **test_set** to calculate the test MSE, which will eventually help us compare the performances of all the models.

7. Finally we refit the model to the **scaled_data.csv** which is our entire data set using the lambda from step 4. We save the coefficient estimates and use it in the *Results* section of the report. 

Our next two methods, Principal Components(PCR) and Partial Least Squares regression(PLSR) are dimension reduction methods performed by **pcr-script.R** and **plsr_regression.R** respectively. The steps for these two regression are very similar to those above, so naturall this outline will not go into as much detail.

1. Load **scaled_credit.csv**, and 'library(pls)' and set the seed, for reproducibility sake.

2. Create a 'x' and 'y' varible from the training set, read in from my Rdata file containing training and testing set indicies. 

3. Run 'pcr()' or 'plsr()' depending on which model you want, and use arguements 'Balance ~ .', 'data=train_set', 'validation = CV' and 'scale = TRUE'.

4. We decide the best model using 'which.min(PCR/PLSR_model$validation$PRESS)' where 'MODEL' is the name of the pcr or plsr model depending on the script.

5. Next we plot the model and save it to a png.

6. Once we identified the best model we use the **test_set** to calculate the test MSE, which will eventually help us compare the performances of all the models.

7. Finally we refit the model to the **scaled_data.csv** which is our entire data set using the lambda from step 4. We save the coefficient estimates and use it in the *Results* section of the report. 