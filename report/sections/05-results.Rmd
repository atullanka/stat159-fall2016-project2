<<<<<<< HEAD
=======
#not sure where to put this stuff

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.cap= 'Estimated Regression Coefficients by Variable and Regression'}

# create a data frame for plotting coefficients.
coef_df <- data.frame(type = colnames(coef_matrix), t(abs(coef_matrix)))
# changing the variable names to show what happened after abs value
colnames(coef_df) <- c('type', 'Intercept', 'Abs(Income)', 'Limit', 'Rating',
                       'Cards', 'Abs(Age)', 'Abs(Education)', 
                       'Abs(GenderFemale)', 'StudentYes', 'MarriedYes', 
                       'EthnicityAsian','EthnicityCaucasian')

# use fnc melt to make easier to plot in ggplot. 
coef_tidy <- melt(coef_df, id.vars = 'type')

# change the variable into a factor 
coef_tidy$type <- factor(coef_tidy$type, levels = coef_tidy$type)

# Plotting the different coefficients in barcharts. 
ggplot(coef_tidy, aes(type,value))+
  geom_bar(aes(fill = type), stat = 'identity')+
  facet_wrap(~variable, scales = 'free')+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  scale_fill_manual(values = c('#fe6f5e', '#ffcc33', '#afe313', '#95e0e8',
                               '#7070cc'))+
  ggtitle(label = 'Estimated Regression Coefficients')
```


```{r results= 'asis', echo = FALSE}
Regression <- c('OLS','Ridge', 'Lasso', 'PCR', 'PLSR')
MSE <- c(ols_mse, ridge_MSE, test_mse, pcr_MSE, mse)
mse_chart <- data.frame(Regression, MSE)
mse_tbl <- xtable(mse_chart,
               caption = 'Test MSE Values for the Regression Techniques',
               digits = 7)

print(mse_tbl, caption.placement = 'top',
      comment = getOption('xtable.comment', FALSE),
      include.rownames = FALSE)
```
>>>>>>> b66e3105969ba1cc4fa9029765893a58795c97f6


# Results

In this section we compare the Lasso Regression with Ridge Regression and the Principal Components Regression with Partial Least Squares Regression.
  
## Lasso and Ridge Regression

Below are the analogous plots for LR and RR. These plots compare log(lambda) values to MSEP.

```{r,echo=FALSE,fig.align='center', fig.cap= "Plotting MSE against lambda values for ridge and lasso regression respectively"}
knitr::include_graphics("../images/Ridge_plot.png")
knitr::include_graphics("../images/Lasso_plot.png")
``` 

The lambda values in LR and RR determine to what extent the Ridge and Lasso Regression models shrink the effect of regression coefficients. This shrinkage subsequently influences the MSEP errors as seen in the plots. We can note that lasso regression works best with very small lambda values (breaks in the plot) while ridge regression increases in error gradually (continuity). Since Lasso only uses a subset of the coefficient vector while Ridge does not, Lasso involves fewer predictive elements and so a larger tuning (lambda) values would more heavily influence the prediction, and thus the MSE.   
    
## PCR and PLSR

Below are the plots for cross-validation for PCR and PLSR. They compare the number of components used to the cross-validation MSE (MSEP). 
    
```{r,echo=FALSE,fig.align='center', fig.cap= "Validation plots from PC and PLS regression respectively"}
knitr::include_graphics("../images/Pcr_validationplot.png")
knitr::include_graphics("../images/Plsr_validationplot.png")
``` 

How the dotted red line fits the solid black line indicates how accurate this regression model is for an arbitrary data set. From the graph, it appears as though PLSR better matches up the expected error with the training set error since the lines are further apart for PCR. However, PLSR minimizes error (MSEP) using fewer components than does PCR, and so there is a potential tradeoff in uncertainty, and hence the MSE is larger for PLSR.
    
## Additional Plots

```{r, results = "asis", echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}
load('../data/OLS.Rdata')
load('../data/PCR.Rdata')
load('../data/Lasso.Rdata')
load('../data/Ridge.Rdata')
load('../data/PLSR.Rdata')
library(xtable)

coef_matrix <- matrix(data = c(as.numeric(OLS_summary$coefficients), 
                                  as.numeric(ridge_coef_full),
                                  as.numeric(lasso_coef),0, 
                                  as.numeric(pcr_coef_full), 0, 
                                  as.numeric(coeff)), 
                                  nrow = 12, ncol = 5)

colnames(coef_matrix) <- c('OLS', 'Ridge', 'Lasso', 'PC', 'PLS')
rownames(coef_matrix) <- c('Intercept', 'Income', 'Limit', 'Rating', 'Cards',
                           'Age', 'Education', 'GenderFemale', 'StudentYes',
                           'MarriedYes', 'EthnicityAsian',
                           'EthnicityCaucasian')
print(xtable(coef_matrix, caption = 'Regression Coefficients', digits = 4),
      type = 'latex', comment = FALSE)

```

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.cap= 'Estimated Regression Coefficients by Variable and Regression'}

# create a data frame for plotting coefficients.
coef_df <- data.frame(type = colnames(coef_matrix), t(abs(coef_matrix)))
# changing the variable names to show what happened after abs value
colnames(coef_df) <- c('type', 'Intercept', 'Abs(Income)', 'Limit', 'Rating',
                       'Cards', 'Abs(Age)', 'Abs(Education)', 
                       'Abs(GenderFemale)', 'StudentYes', 'MarriedYes', 
                       'EthnicityAsian','EthnicityCaucasian')

# use fnc melt to make easier to plot in ggplot. 
coef_tidy <- melt(coef_df, id.vars = 'type')

# change the variable into a factor 
coef_tidy$type <- factor(coef_tidy$type, levels = coef_tidy$type)

# Plotting the different coefficients in barcharts. 
ggplot(coef_tidy, aes(type,value))+
  geom_bar(aes(fill = type), stat = 'identity')+
  facet_wrap(~variable, scales = 'free')+
  ggtitle(label = 'Estimated Regression Coefficients by Variable and Regression')

```
